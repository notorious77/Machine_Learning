---
title: "Peer Assignment Machine Learning"
author: "Rodney Waiters"
date: "December 15, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load the data

After loading the training data, the dim() command show that thre are 160 columns and 19,622 rows.

```{r cache=TRUE, echo=FALSE}
library(caret);library(rpart);library(rpart.plot);
library(ROCR);library(e1071);library(randomForest);
#source("performance_plot_utils.R")
#setwd("\\\\admin-fs1/Users/rwaiters/My Documents/R/RProgamming/MachineLearning/")
train_raw<-read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""," ")) #stringsAsFactors = FALSE)
test_raw<-read.csv("pml-testing.csv", stringsAsFactors = FALSE)

dim(train_raw)
```
Remove Zero Columns

The next code segment we are removing all the columns that sum to zero.  We know that these columns have no data.  The dim() function on this data set shows that there are 60 remaining columns and still 19,622 rows.
```{r cache=TRUE}
trainData <- train_raw[, colSums(is.na(train_raw)) == 0]
testData <- test_raw[, colSums(is.na(test_raw)) == 0]
dim(trainData); dim(testData);
```
Remove first six columns since they have no predictive qualities.
```{r cache=TRUE}
train_in<-trainData[,-c(1:7)]
testing<-testData[,-c(1:7)]
dim(train_in)
```

Check for variables that have a near zero variables.

```{r, echo=FALSE}
suppressWarnings(library(caret))
library(caret)
nzv_train<-nearZeroVar(train_in, freqCut = 95/5, uniqueCut = 10,saveMetrics = TRUE)
nzv_train[1:10,]

nzv_test<-nearZeroVar(testing, freqCut = 95/5, uniqueCut = 10,saveMetrics = TRUE)
nzv_test[1:10,]

```
It appears that the none of the variables meet the frequency cut of 95/5 since we still have 53 variables and 19,622 observations.
```{r}
dim(train_in)
```
Partition the data
```{r cache=TRUE}
set.seed(7777)
trainPart<-createDataPartition(train_in$classe, p = .7, list = FALSE)

training<-train_in[trainPart,]
validData<-train_in[-trainPart,]
#training_labels <- as.factor(training$classe)



```

The training and validation data have been divided into two data sets.  Training with 13,737 observations and 53 variables.  Validataion with 5,885 observations and 53 variables.
```{r echo=FALSE}
dim(training);dim(validData);
```

Feature Selection

The figure below shows the features that we believe that may have some predictive qualities.

```{r cache=TRUE ,echo = FALSE}
cont<-trainControl(method = "cv", number = 5)
fit0<-train(classe~.,  data = training, method = "rpart", trControl = cont)
importance<-varImp(fit0, scale = FALSE)
plot(importance,top=10)
```

GGVIS

I wanted to see if there were any clustering properties in the data set.  The following chart really didn't show anything, but I left it in to show that not every piece of data will yield meaningful results.
```{r}
library(ggvis)
training %>% ggvis(~roll_belt, ~yaw_belt, fill = ~factor(classe)) %>% layer_points()
```

Model Selection

```{r cache=TRUE}
RF_model<-randomForest(classe~., data = training, importance = T, ntree = 500)
RF_model
```

Prediction Model
```{r cache=TRUE}
set.seed(7777)
prediction =predict(RF_model, newdata = validData )
confusionMatrix(prediction, validData$classe)
```

Test Submission

Now we use the random forest model that we trained on the validation data to predict on the test set.

```{r echo=FALSE}
library(caret)
library(randomForest)
test_results <- predict(RF_model, newdata =testing)
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(test_results)
```
